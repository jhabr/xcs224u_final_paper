\begin{abstract}

\begin{itemize}
  In this work we present a new model for a grounded communication task; identifying colors  generated from descriptions. This task uses two recurrent neural network cells which act as the speaker and listener. We build on previous work and introduce two novel methods for embedding our recurrent neural network inputs; a convolutional encoder for colors and a transformer network for encoding utterances. We show that coupling these two encoders with a recurrent sequence-to-sequence architectures can sometimes achieve state-of-the-art results in both listener accuracy and BLEU score, but the gains are often minimal with great computational cost and highly prone to overfitting. In addition, our paper demonstrates that using complex encoders such as transformers or convolutional neural networks on small high dimensional datasets don't clearly outperform simpler models such as a Fourier Transform or GLoVe.

\end{itemize}

\end{abstract}