\section{Related Work}

\citep{monroe-2016-compositional} build a model that is capable of producing novel color descriptions (“greenish”, “bright”, "faded teal") not seen during training. This is a challenging task as color descriptions can be vague, compositionally complex and denotationally rich. They consider color generation as a grounded language modelling problem and present an effective approach to perform this task using LSTM RNN and color representations transformed with Fourier-transformation used in computer vision for feature representations. The study of \citep{monroe-2017-colors} is continuation of the work of \citep{monroe-2016-compositional}. The autors create a much larger task-oriented dialog corpus \citep{moroe-2017-colors-reference-dataset} where a listener has identified a color based on short descriptions from a speaker. This dataset contains color descriptions using a variety of language use such as negations, comparatives, superlatives, metaphor, and shared associations. The authors show that a model, a combination of a listener RNN and a speaker RNN, performs better although both perform purely alone. They also observe significant improvement of the listener RNN with pragmatic reasoning and especially in the hardest cases: 1) contexts with very similar colors; 2) target colors that the referring expressions fail to identify.

\par
Another challening machine learning task that requires grounding and complex language understanding is the task of describting images. \citep{andreas-2016-reasoning} present a model with a feature-driven architecture using a simple NN listener and speaker. The model is capable of actively reasoning about the listener’s behaviour when selecting utterances describing images. The model produces a pragmatic description of an image caption that is informative, fluent, concise and understanding the listeners’ behaviour. The model is able to outperform the baseline system by being able to generate image descriptions that are correctly interpreted 17% more often.

\par
In recent year transformer based models have been the leading models in understnading context and complex language. The famous work of \citep{vaswani-2017-attention} has demonstrated that attention can significantly improve the performance of those models. We use four such models which improve on different inefficiencies of BERT. To solve the data corruption and discrepancy problems of the BERT-based transformers caused by [MASK] token, XLNet \citep{yang-2019-xlnet} uses an autoregressive language model, and ELECTRA \citep{clark-2020-electra} uses a novel training scheme reminiscent of a Generative Adversarial Network (GAN). RoBERTa \citep{liu-2019-roberta} implements a new dynamic masking strategy to be able to pretrain on more data.

\par
The ability of humans to describe images with natural language and hence connect those two dimensions in a fluent way is the guiding motivation for the exploration of the multimodal space. The work by \citep{karpathy-2014-image_descriptions} focuses on generating concise descriptions from images by using deep neural networks. To create dense descriptions of visual data, the authors present a new multimodal recurrent neural network generative model which uses a region convolutional neural network (RCNN) pre-trained on ImageNet \citep{deng-2009-imagenet} to detect objects in every image. The generative model was trained to predict the next word given the previous context word and the probability distribution of the words in the vocabulary, denoting the start and end of the sequence with a special START and END token. The work of \citep{monroe-2017-colors} can be seen as a special case of image captioning, with the encoder part enriched with Fourier-transformed color representation. Lastly, a more recent work published by \citep{openai-2020-dalle} presents a generative multimodal model called DALL·E, capable of generating or completing images from text. It uses a trained transformer to autoregressively model the text and image tokens as a single stream of data.

\par
In our work, we enhanced the color representation of the original model by \citep{monroe-2017-colors}. with an encoder based on convolutional neural networks like ResNet-18 and encoded the word representations as pre-trained and contextual word embeddings based on transformer models like BERT, ELECTRA, RoBERTa or XLNet. Since ResNet-18 and ResNet-50 have been pre-trained on the ImageNet \citep{imagenet-2019-dataset}, we evaluate their embedded richer grounding color information compared to pure Fourier-transformed color representations.
