\section{Related Work}

\citep{monroe-2016-compositional} build a model that is capable of producing novel color descriptions (“greenish”, “bright”, "faded teal") not seen during training. This is a challenging task as color descriptions can be vague, compositionally complex and denotationally rich. They consider color generation as a grounded language modelling problem and present an effective approach to perform this task using LSTM RNN and color representations transformed with Fourier-transformation used in computer vision for feature representations. The study of \citep{monroe-2017-colors} is continuation of the work of \citep{monroe-2016-compositional}. The autors create a much larger task-oriented dialog corpus \citep{moroe-2017-colors-reference-dataset} where a listener has identified a color based on short descriptions from a speaker. This dataset contains color descriptions using a variety of language use such as negations, comparatives, superlatives, metaphor, and shared associations. The authors show that a model, a combination of a listener RNN and a speaker RNN, performs better although both perform purely alone. They also observe significant improvement of the listener RNN with pragmatic reasoning and especially in the hardest cases: 1) contexts with very similar colors; 2) target colors that the referring expressions fail to identify.

\par
Another challening machine learning task that requires contextual grounding and complex language is the task of ddescribting images. \citep{andreas-2016-reasoning} present a model with a feature-driven architecture using a simple NN listener and speaker. The model is capable of actively reasoning about the listener’s behaviour when selecting utterances describing images. The model has to produce a pragmatic description of an image caption that is informative, fluent, concise and understanding the listeners’ behaviour. The model is able to outperform the baseline system by being able to generate image descriptions that are correctly interpreted 17% more often. 

\par
Four different transformers that make use of attention which improve on different inefficiencies of BERT. To solve the data corruption and discrepancy problems of the BERT-based transformers caused by [MASK] token, XLNet uses an autoregressive language model, and ELECTRA uses a novel training scheme reminiscent of a Generative Adversarial Network (GAN). RoBERTa implements a new dynamic masking strategy to be able to pretrain on more data.

\par
Related work tackles the challenge of connecting natural language understanding and vision in a variety of different ways. \citep{karpathy-2014-image_descriptions} focuses on generating concise descriptions from images by using deep neural networks. The objects were detected in every image with a region convolutional neural network, which was  pre-trained and fine-tuned based on ImageNet. DALL·E makes the connection from the other direction generating complete images from text by using decoder only achriteachure. It uses a trained transformer to autoregressively model the text and image tokens as a single stream of data. Lastly, the work of \citep{monroe-2017-colors} can be seen as a special case of image captioning, with the encoder part enriched with Fourier-transformed color representation.

\par
In our work, we enhanced the color representation of a simplified version of the original model by \citep{monroe-2017-colors} consisting of an encoder to encode the colors and a decoder to output a color description with pre-trained and contextual word embeddings -and pre-trained convolutional model ResNet18, which should contain richer grounding color information compared with Fourier-transformed color representations.

% \begin{itemize}
%   \item based on lit review
%   \item organize paper into groups I want to cover =$>$ relation to my work
%   \item for each group, articulate thematic unity, paper achievements =$>$ relation to my work =$>$ gives context, differentiation from prior work
% \end{itemize}

% Papers:

% \begin{itemize}
%   \item Attention is all you need: \citep{vaswani-2017-attention}.
%   \item Colors in Context: \citep{monroe-2017-colors}.
%   \item Compositional Color Descriptions: \citep{monroe-2016-compositional}.
%   \item GPT-3: \citep{brown-2020-gpt3}.
%   \item Generating Image Descriptions: \citep{karpathy-2014-image_descriptions}.
%   \item OpenAI Dall-E: \citep{openai-2020-dalle}.
%   \item Huggingface: \citep{wolf-2019-huggingface}.
%   \item ELECTRA: \citep{clark-2020-electra}.
%   \item XLNet: \citep{yang-2019-xlnet}.
%   \item RoBERTa: \citep{liu-2019-roberta}.
%   \item Contextual Word Representation: \citep{smith-2019-contextual}.
%   \item Reasoning About Pragmatics with Neural Listeners and Speakers: \citep{andreas-2016-reasoning}.
%   \item LXMERT: \citep{tan-2019-lxmert}.
%   \item ImageNet: \citep{deng-2009-imagenet}.
%   \item ImageNet dataset: \citep{imagenet-2019-dataset}.
% \end{itemize}
