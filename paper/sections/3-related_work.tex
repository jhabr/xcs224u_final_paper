\section{Related Work}

The study of \citep{monroe-2017-colors} set the cornerstone for this work on grounding-based color description. The core concept represents the fact that human language is situational and often contains assumptions about the common world understanding, specific knowledge about a given topic of conversation and its context. To address their hypothesis, the authors present a color prediction system based on two recurrent neural network classifiers, a speaker and a listener, unified by a recursive pragmatic reasoning framework \citep{monroe-2017-colors}.

\par
Four different transformers that make use of attention which improve on different inefficiencies of BERT. To solve the data corruption and discrepancy problems of the BERT-based transformers caused by [MASK] token, XLNet uses an autoregressive language model, and ELECTRA uses a novel training scheme reminiscent of a Generative Adversarial Network (GAN). RoBERTa implements a new dynamic masking strategy to be able to pretrain on more data.

\par
The ability of humans to describe images with natural language and hence connect those two dimensions in a fluent way is the guiding motivation for the exploration of the multimodal space. The work by \citep{karpathy-2014-image_descriptions} focuses on generating concise descriptions from images by using deep neural networks. To create dense descriptions of visual data, the authors present a new multimodal recurrent neural network generative model which uses a region convolutional neural network (RCNN) pre-trained on ImageNet \citep{deng-2009-imagenet} to detect objects in every image. The generative model was trained to predict the next word given the previous context word and the probability distribution of the words in the vocabulary, denoting the start and end of the sequence with a special START and END token. The work of \citep{monroe-2017-colors} can be seen as a special case of image captioning, with the encoder part enriched with Fourier-transformed color representation. Lastly, a more recent work published by \citep{openai-2020-dalle} presents a generative multimodal model called DALLÂ·E, capable of generating or completing images from text. It uses a trained transformer to autoregressively model the text and image tokens as a single stream of data.

\par
In our work, we enhanced the color representation of the original model by \citep{monroe-2017-colors}. with an encoder based on convolutional neural networks like ResNet18 and encoded the word representations as pre-trained and contextual word embeddings based on transformer models like BERT, ELECTRA, RoBERTa or XLNet. Since ResNet18 and ResNet50 have been pre-trained on the ImageNet \citep{imagenet-2019-dataset}, we evaluate their embedded richer grounding color information compared to pure Fourier-transformed color representations.
