\section{Introduction}

Human language is highly contextual and often presumes prior knowledge of a given topic. Meaning of words is mostly implicit and is inferred from context. Such interpretations are difficult for machine learning models as context is not only represented by words but it also assumes as a common world understanding \citep{monroe-2017-colors}. In reality, words can have different meanings in different contexts and hence cannot be represented with a single vector. This obvervation let to the introduction of contextual word representations based on word tokens and has been widely adopted by attention-based models and especially transformers like e.g. BERT, ELECTRA or XLNet \citep{smith-2019-contextual,vaswani-2017-attention}.

\par
Visual referring tasks like color or image descriptions are very suitable for language understanding tasks as they exercise this complex process of grounding in the environment and in our mental models of each other \citep{monroe-2017-colors, karpathy-2014-image_descriptions}.

\par
The central hypothesis of this paper is that rich color encondings as represented in convolutional neural networks (CNN) as well pre-trained and contextual word embeddings as used by attention based architectures such as transformers will help improve the performance on the color description task presented in \citep{monroe-2017-colors}. We hypothesize that those higher dimensional and more complex representations encode some useful information about the world and the given context from which a grounded communication task will be able to benefit.

\par
To address our hypothesis we build upon the sequence-to-sequence model presented by \citep{monroe-2017-colors}. As a first step we add a transfomer-based color descriptions encoder. Inspired by feature representations in computer vision, we then complement the model architecture with a ResNet color encoder pre-trained on ImageNet. This helps us to embed additional grounding information. To find the best performing setup, we finally perform a number of different operations on the transfomer encoder's hidden states, accompanied by a final fine-tuning along a defined hyperparameter space. The code of our work can be viewed on GitHub\footnote{\url{https://github.com/jhabr/xcs224u_final_paper}}.
