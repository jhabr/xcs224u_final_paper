\section{Models}

This section describes the models in more detail.

\subsection{Target Architecture}

\begin{figure}[ht]
\centering
\includegraphics[width=\columnwidth]{assets/target_architecture.pdf}
\caption[Target Architecture]
{Target architecture with pre-trained convolutional encoder and transformer generated contextual word embeddings.}
\label{overview}
\end{figure}

Figure \ref{overview} depicts our target architecture developed for our experiments. It consists of three core components. The pre-trained convolutional encoder is our method for extracting rich color embeddings. This could be any pre-trained convolutional model such as VGG19 or Resnet-152, it just requires that we use one of the last hidden layers before the softmax projection as our encode color embeddings. We use the last layer of ResNet-18 in our experiments.

\par
The transformer generated contextual embeddings are any word embeddings generated by a transformer. This could be embeddings generated by ELECTRA, XLNet, BERT or any other transformer variants. These are used to encode the words into a semantic vector space where their projection will be dependent on their context. Some descriptions are single words, thus contextual information will not be encoded.

\par
The generated color and contextual word embeddings are then brought together using a standard sequence-to-sequence architecture. The color embeddings are sent through an RNN unit such as a Gated Recurrent Unit (GRU) or Long Short Term Memory Unit (LSTM). The target color is sent through the RNN layer last so that its information encoded in the hidden state is \emph{closest} to the decoder. The decoder, another RNN unit, then takes the hidden encoder's last hidden layer input and starts to decode the input utterance. For each timestep of the decoder we concatenate the target colors embedding, generated by the ResNet-18, with the input word. Once the output RNN reaches its specified stop token we stop generating text.

\subsection{Experiment Setup}
To address our hypotheses we use different model compositions which are based on our target architecture.

\textbf{Baseline}:
As a baseline model, we use our implementation of the encoder-decoder architecture model based on a single layer of GRU for both the encoder and decoder. The colors are encoded using Fourier transformation.

\textbf{Part one}:
We use Fourier encoded context colors with pre-trained and contextual embeddings from the following transformer models: BERT, XLNet, ELECTRA, RoBERTa.

\textbf{Part two}:
We use ResNet-18 and ResNet-50 for contextual color representation while keeping the same model embeddings.

\textbf{Part three}:
We build on part two and combine different hidden layers of the transformer models. Namely, we concatenate the last four hidden layers, sum the last four hidden layers and extract the last and second-to-last hidden layer.
