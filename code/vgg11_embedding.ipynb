{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.colors import ColorsCorpusReader\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from collections import Counter\n",
    "import re\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.fft import fft\n",
    "import colorsys\n",
    "from itertools import product\n",
    "from utils.torch_color_describer import (ContextualColorDescriber, create_example_dataset)\n",
    "import utils\n",
    "from utils.utils import UNK_SYMBOL\n",
    "import utils.model_utils as mu\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline.model import (\n",
    "    BaselineTokenizer, BaselineColorEncoder,\n",
    "    BaselineEmbedding, BaselineDescriber, GloVeEmbedding\n",
    ")\n",
    "from experiment.word_embeddings.helper import Embedding, EmbeddingType\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizer, BertModel,\n",
    "    XLNetTokenizer, XLNetModel,\n",
    "    RobertaTokenizer, RobertaModel,\n",
    "    ElectraTokenizer, ElectraModel,    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.utils.fix_random_seeds()\n",
    "COLORS_SRC_FILENAME = os.path.join(\n",
    "    \"data\", \"colors\", \"filteredCorpus.csv\")\n",
    "\n",
    "dev_corpus = ColorsCorpusReader(\n",
    "    COLORS_SRC_FILENAME,\n",
    "    word_count=None,\n",
    "    normalize_colors=True)\n",
    "\n",
    "dev_examples = list(dev_corpus.read())\n",
    "dev_rawcols, dev_texts = zip(*[[ex.colors, ex.contents] for ex in dev_examples])\n",
    "dev_rawcols_train, dev_rawcols_test, dev_texts_train, dev_texts_test = \\\n",
    "    train_test_split(dev_rawcols, dev_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46994, 46994)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_rawcols), len(dev_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/yanjiang/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU(inplace=True)\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import colorsys\n",
    "\n",
    "def load_model_feature_extractor(model_arch='vgg11'):\n",
    "    model = torch.hub.load('pytorch/vision:v0.6.0', model_arch, pretrained=True)\n",
    "    model.classifier = model.classifier[:-3]\n",
    "    feature_extractor = model\n",
    "    return feature_extractor\n",
    "\n",
    "feature_extractor = load_model_feature_extractor()\n",
    "feature_extractor.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Color Rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_color_to_rgb(color):\n",
    "    # Convert from HLS to RGB\n",
    "    rgb = colorsys.hls_to_rgb(color[0],color[1],color[2])\n",
    "    return rgb\n",
    "\n",
    "def convert_to_imagenet_input(hsl):\n",
    "    rgb = convert_color_to_rgb(hsl)\n",
    "    r = torch.full((224,224),rgb[0]).unsqueeze(2)\n",
    "    g = torch.full((224,224),rgb[1]).unsqueeze(2)\n",
    "    b = torch.full((224,224),rgb[2]).unsqueeze(2)\n",
    "    expanded_rep = torch.cat((r,g,b),2)    \n",
    "#     print(type(expanded_rep), expanded_rep.shape)\n",
    "    expanded_rep = expanded_rep.permute(2,1,0).unsqueeze(0)\n",
    "    return expanded_rep\n",
    "\n",
    "def convert_color_tuple(colors):\n",
    "    converted_colors = [[convert_to_imagenet_input(col) for col in cols] for cols in colors ] \n",
    "#     print (len(converted_colors), len(converted_colors[0]), converted_colors[0][0].shape)\n",
    "    return converted_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_batch(extractor, examples):\n",
    "    output = extractor(examples)\n",
    "    shape = output.shape\n",
    "    output = output.reshape((shape[0],shape[1]))\n",
    "#     print(\"shape\", shape, \"reshape\", output.shape)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_representation(rawcols):\n",
    "\n",
    "    converted_data = convert_color_tuple(rawcols)\n",
    "    print(\"input:\", len(rawcols), \"length of converted_data:\", len(converted_data))\n",
    "    \n",
    "    extracted_features  = []\n",
    "    with torch.no_grad():\n",
    "        for colors in converted_data:\n",
    "            # Convert to 3x224x224 matrix\n",
    "            cols_batch = torch.cat((colors[0],\n",
    "                                colors[1],\n",
    "                                colors[2]))\n",
    "#             print(\"cols_batch:\", cols_batch.shape)\n",
    "\n",
    "            # Run color through the feature extractor\n",
    "            batch_extraction = extract_features_from_batch(feature_extractor, cols_batch)\n",
    "#             print(\"batch_extraction:\", batch_extraction.shape)\n",
    "\n",
    "            # convert to numpy array\n",
    "            batch_extraction_np = batch_extraction.numpy()\n",
    "#             print(\"batch_extraction_np\", len(batch_extraction_np), type(batch_extraction_np[0]), batch_extraction_np[0].size)\n",
    "\n",
    "            # append to list\n",
    "            extracted_features.append(batch_extraction)\n",
    "\n",
    "            # Print some stats \n",
    "            length = len(extracted_features)\n",
    "            print(length)\n",
    "            if length%100==0:\n",
    "                total_size = sys.getsizeof(extracted_features)\n",
    "                print(f\"Running batch number: {length}, Size of array: {total_size/(1024**2)} Megabytes\")\n",
    "\n",
    "    return extracted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_rawcols[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 3 length of converted_data: 3\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "x = get_color_representation(dev_rawcols[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, torch.Size([3, 4096]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x), x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(extracted_features), len(extracted_features[0]), len(extracted_features[0][0]), type(extracted_features[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extracted_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5cc4d4198b72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mextracted_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"data/colors/vgg11_color_embeddings_test.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'extracted_features' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "pickle.dump( extracted_features, open( \"data/colors/vgg11_color_embeddings_test.pickle\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = []\n",
    "with (open(\"data/colors/resnet18_color_embeddings.pickle\", \"rb\")) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            objects.append(pickle.load(openfile))\n",
    "        except EOFError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_encoder = BaselineColorEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(tokenizer, include_position=False):    \n",
    "    rawcols, texts = zip(*[[ex.colors, ex.contents] for ex in examples])\n",
    "\n",
    "    raw_colors_train, raw_colors_test, texts_train, texts_test = \\\n",
    "        train_test_split(rawcols, texts)\n",
    "\n",
    "    tokens_train = [\n",
    "        mu.tokenize_colour_description(text, tokenizer, include_position) for text in texts_train\n",
    "    ]\n",
    "#     colors_train = [\n",
    "#         color_encoder.encode_color_context(colors) for colors in raw_colors_train\n",
    "#     ]\n",
    "    colors_train = [\n",
    "        get_color_representation(colors) for colors in raw_colors_train\n",
    "    ]\n",
    "\n",
    "    return colors_train, tokens_train, raw_colors_test, texts_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "roberta_model = RobertaModel.from_pretrained('roberta-base') \n",
    "colors_train, tokens_train, raw_colors_test, texts_test = create_data(roberta_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(colors_train), len(colors_train[0]), len(colors_train[0][0]), type(colors_train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time roberta_embeddings, roberta_vocab = mu.extract_input_embeddings(texts_test, roberta_model, roberta_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roberta_embedding(dev_texts, if_contextual=False):\n",
    "    roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    roberta_model = RobertaModel.from_pretrained('roberta-base')    \n",
    "    \n",
    "    roberta_embeddings, roberta_vocab = mu.extract_input_embeddings(dev_texts, roberta_model, roberta_tokenizer)\n",
    "    roberta_contextual_embeddings, roberta_contextual_vocab = mu.extract_positional_embeddings(dev_texts, roberta_model, roberta_tokenizer)\n",
    "    \n",
    "    if not if_contextual:\n",
    "        return (roberta_vocab, roberta_embeddings)\n",
    "    else:\n",
    "        return roberta_contextual_vocab, roberta_contextual_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = BaselineDescriber(\n",
    "    roberta_vocab,\n",
    "    embedding=roberta_embeddings,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _ = baseline_model.fit(colors_train[:5], tokens_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(trained_model, tokenizer, color_seqs_test, texts_test):\n",
    "    tok_seqs = [mu.tokenize_colour_description(text, tokenizer) for text in texts_test]\n",
    "    col_seqs = [color_encoder.encode_color_context(colors) for colors in color_seqs_test]\n",
    "\n",
    "    return trained_model.evaluate(col_seqs, tok_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(baseline_model, roberta_tokenizer, raw_colors_test, texts_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
