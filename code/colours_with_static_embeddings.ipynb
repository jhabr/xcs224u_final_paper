{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__authors__ = \"Anton Gochev, Jaro Habr, Yan Jiang, Samuel Kahn\"\n",
    "__version__ = \"XCS224u, Stanford, Spring 2021\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colours with static embeddings\n",
    "\n",
    "1. [Setup](#Setup)\n",
    "1. [Dataset](#Dataset)\n",
    "    1. [Filtered Corpus](#Filtered-Corpus)\n",
    "    1. [Bake-Off Corpus](#Bake-Off-Corpus)\n",
    "1. [Baseline-System](#Baseline-System)\n",
    "1. [Experiments](#Experiments)\n",
    "  1. [BERT Embeddings](#BERT-Embeddings)\n",
    "  2. [XLNet Embeddings](#XLNet-Embeddings)\n",
    "  3. [RoBERTa Embeddings](#RoBERTa-Embeddings)\n",
    "  4. [ELECTRA Embeddings](#ELECTRA-Embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook explores the performance of the basemodel with using different pre-trained static embeddings extracted from transformers such as BERT, XLNet, RoBERTa, ELECTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.colors import ColorsCorpusReader\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from utils.torch_color_describer import ContextualColorDescriber, create_example_dataset\n",
    "import utils.utils\n",
    "from utils.utils import UNK_SYMBOL, START_SYMBOL, END_SYMBOL\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatch\n",
    "import numpy as np\n",
    "from baseline.model import (\n",
    "    BaselineTokenizer, BaselineColorEncoder,\n",
    "    BaselineEmbedding, BaselineDescriber, \n",
    "    GloVeEmbedding, BaselineLSTMDescriber    \n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizer, BertModel,\n",
    "    XLNetTokenizer, XLNetModel,\n",
    "    RobertaTokenizer, RobertaModel,\n",
    "    ElectraTokenizer, ElectraModel,    \n",
    ")\n",
    "\n",
    "import utils.model_utils as mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exploration of the dataset counts the examples for different classes and plots the words distribition in order to see any data imbalance issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtered Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filtered corpus is the full dataset used in assignment 4. The following code looks at the composition of the dataset, the number of example in each condition as well as the word count used in the color descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS_SRC_FILENAME = os.path.join(\n",
    "    \"data\", \"colors\", \"filteredCorpus.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ColorsCorpusReader(\n",
    "    COLORS_SRC_FILENAME,\n",
    "    word_count=None,\n",
    "    normalize_colors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = list(corpus.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46994"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_examples = mu.extract_colour_examples(examples, from_word_count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the datasets (training and bake-off) in more details refer to [colors_in_context.ipynb](colors_in_context.ipynb). The notebook shows the distribution of the colours examples among the different splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bake-Off Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code analyses the bake-off dataset. We will look at the number of examples for each of the conditions as well as the word count used to described the colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAKE_OFF_COLORS_SRC_FILENAME = os.path.join(\n",
    "    \"data\", \"colors\", \"cs224u-colors-bakeoff-data.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bake_off_corpus = ColorsCorpusReader(\n",
    "    BAKE_OFF_COLORS_SRC_FILENAME,\n",
    "    word_count=None,\n",
    "    normalize_colors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bake_off_examples = list(bake_off_corpus.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2031"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bake_off_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline-System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This baseline system is based on assignment 4 and we use different token embeddings and sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline system development - dev dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tiny dataset is used for baseline model development and fast testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dev_data():\n",
    "    dev_color_seqs, dev_word_seqs, dev_vocab = create_example_dataset(\n",
    "        group_size=50,\n",
    "        vec_dim=2\n",
    "    )\n",
    "\n",
    "    dev_colors_train, dev_colors_test, dev_words_train, dev_words_test = \\\n",
    "        train_test_split(dev_color_seqs, dev_word_seqs)\n",
    "    \n",
    "    return dev_vocab, dev_colors_train, dev_words_train, dev_colors_test, dev_words_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_vocab, dev_colors_train, dev_tokens_train, dev_colors_test, dev_texts_test = \\\n",
    "    create_dev_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training - full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full color context dataset is used for final baseline model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_encoder = BaselineColorEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(tokenizer, include_position=False,include_conv_embeddings=False):\n",
    "    if include_conv_embeddings==False:\n",
    "        rawcols, texts = zip(*[[ex.colors, ex.contents] for ex in examples])\n",
    "    \n",
    "    raw_colors_train, raw_colors_test, texts_train, texts_test = \\\n",
    "        train_test_split(rawcols, texts)\n",
    "    \n",
    "    raw_colors_train = raw_colors_train\n",
    "    texts_train = texts_train\n",
    "\n",
    "    tokens_train = [\n",
    "        mu.tokenize_colour_description(text, tokenizer) for text in texts_train\n",
    "    ]\n",
    "    colors_train = [\n",
    "        color_encoder.encode_color_context(colors) for colors in raw_colors_train\n",
    "    ]\n",
    "\n",
    "    return colors_train, tokens_train, raw_colors_test, texts_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bakeoff_data():    \n",
    "    return zip(*[[ex.colors, ex.contents] for ex in bake_off_examples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook contains results produced with the baseline architecture and with different static embeddings and RNN cells. We use GRU and LSTM. LSTM shows a slightly better accuracy results and worse BLEU scores. \n",
    "\n",
    "In order to run the experiments using different embeddings:\n",
    "- for GRU - use BaselineDescriber\n",
    "- for LSTM - use BaselineLSTMDescriber\n",
    "\n",
    "In the code in this notebook this is the only change you have to make to experiment with one or the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from the experiments:\n",
    "\n",
    "| Model | Protocol | Cell | Training Results | Bake-off Results |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| BERT 'bert-base-cased' | GRU | Stopping after epoch 54. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 115.87914156913757 CPU times: user 2h 4min 43s, sys: 1h 1min 36s, total: 3h 6min 19s Wall time: 1h 3min 47s | {'listener_accuracy': 0.8282407013362839, 'corpus_bleu': 0.4614753491619744} | {'listener_accuracy': 0.9034958148695224, 'corpus_bleu': 0.6637981882180088} |\n",
    "| BERT 'bert-base-cased' | LSTM | Stopping after epoch 83. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 76.14108896255493 CPU times: user 3h 3min 25s, sys: 1h 27min 37s, total: 4h 31min 2s  Wall time: 1h 30min 36s | {'listener_accuracy': 0.8422844497404034, 'corpus_bleu': 0.4628617489092576} | {'listener_accuracy': 0.9084194977843427, 'corpus_bleu': 0.663598646435816} |\n",
    "| XLNet 'xlnet-base-cased' | GRU | Stopping after epoch 55. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 117.36168694496155 CPU times: user 2h 18min 38s, sys: 1h 7min 4s, total: 3h 25min 43s Wall time: 1h 9min 34s | {'listener_accuracy': 0.8243254745084688, 'corpus_bleu': 0.4290345340732608} | {'listener_accuracy': 0.9034958148695224, 'corpus_bleu': 0.6462354138567806} |\n",
    "| XLNet 'xlnet-base-cased' | LSTM | Stopping after epoch 61. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 107.04569149017334 CPU times: user 2h 44min 52s, sys: 1h 13min 37s, total: 3h 58min 29s Wall time: 1h 20min 2s | {'listener_accuracy': 0.8394757000595795, 'corpus_bleu': 0.43106009125838857} | {'listener_accuracy': 0.9148202855736091, 'corpus_bleu': 0.6450470273203873} |\n",
    "| RoBERTa 'roberta-base' | GRU | Stopping after epoch 71. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 88.95393252372742 CPU times: user 2h 26min 29s, sys: 1h 11min 9s, total: 3h 37min 38s Wall time: 1h 10min 30s | {'listener_accuracy': 0.8160694527193804, 'corpus_bleu': 0.43914107019568926} | {'listener_accuracy': 0.8936484490398818, 'corpus_bleu': 0.6365891729377956} |\n",
    "| RoBERTa 'roberta-base' | LSTM | Stopping after epoch 61. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 111.48405265808105 CPU times: user 2h 41min 37s, sys: 1h 11min 45s, total: 3h 53min 23s Wall time: 1h 15min 32s | {'listener_accuracy': 0.8200697931738872, 'corpus_bleu': 0.39418980395247344} | {'listener_accuracy': 0.8941408173313639, 'corpus_bleu': 0.5649086521910678} |\n",
    "| ELECTRA 'google/electra-small-discriminator' | GRU | Stopping after epoch 55. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 112.70061588287354 CPU times: user 1h 18min 13s, sys: 33min 6s, total: 1h 51min 20s Wall time: 37min 34s | {'listener_accuracy': 0.8459443356881436, 'corpus_bleu': 0.4808836633817978} | {'listener_accuracy': 0.914327917282127, 'corpus_bleu': 0.6813010156866868} |\n",
    "| ELECTRA 'google/electra-small-discriminator' | LSTM | Stopping after epoch 79. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 80.59795188903809 CPU times: user 2h 2min 9s, sys: 47min 40s, total: 2h 49min 49s Wall time: 56min 50s | {'listener_accuracy': 0.8423695633670951, 'corpus_bleu': 0.47554009108512063} | {'listener_accuracy': 0.9187592319054653, 'corpus_bleu': 0.6793120721302932} |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(trained_model, tokenizer, color_seqs_test, texts_test):\n",
    "    tok_seqs = [mu.tokenize_colour_description(text, tokenizer) for text in texts_test]\n",
    "    col_seqs = [color_encoder.encode_color_context(colors) for colors in color_seqs_test]\n",
    "\n",
    "    return trained_model.evaluate(col_seqs, tok_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.88 s, sys: 111 ms, total: 7.99 s\n",
      "Wall time: 8.01 s\n"
     ]
    }
   ],
   "source": [
    "%time colors_train, tokens_train, raw_colors_test, texts_test = create_data(bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.74 ms, sys: 106 µs, total: 3.85 ms\n",
      "Wall time: 3.86 ms\n"
     ]
    }
   ],
   "source": [
    "%time b_raw_colors_test, b_texts_test = create_bakeoff_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.41 s, sys: 12.1 ms, total: 2.42 s\n",
      "Wall time: 2.43 s\n"
     ]
    }
   ],
   "source": [
    "%time bert_embeddings, bert_vocab = mu.extract_input_embeddings(texts_test, bert_model, bert_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Baseline model using BERT pretrained embeddings and vocab*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = BaselineLSTMDescriber(\n",
    "    bert_vocab,\n",
    "    embedding=bert_embeddings,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 83. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 76.14108896255493"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3h 3min 25s, sys: 1h 27min 37s, total: 4h 31min 2s\n",
      "Wall time: 1h 30min 36s\n"
     ]
    }
   ],
   "source": [
    "%time _ = baseline_model.fit(colors_train, tokens_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.4 s, sys: 4.46 s, total: 42.8 s\n",
      "Wall time: 37.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'listener_accuracy': 0.8422844497404034, 'corpus_bleu': 0.4628617489092576}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time evaluate(baseline_model, bert_tokenizer, raw_colors_test, texts_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on bake-off data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.23 s, sys: 455 ms, total: 6.69 s\n",
      "Wall time: 6.05 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'listener_accuracy': 0.9084194977843427, 'corpus_bleu': 0.663598646435816}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time evaluate(baseline_model, bert_tokenizer, b_raw_colors_test, b_texts_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLNet Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlnet_tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "xlnet_model = XLNetModel.from_pretrained('xlnet-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_train, tokens_train, raw_colors_test, texts_test = create_data(xlnet_tokenizer)\n",
    "\n",
    "b_raw_colors_test, b_texts_test = create_bakeoff_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.38 s, sys: 18 ms, total: 2.4 s\n",
      "Wall time: 2.41 s\n"
     ]
    }
   ],
   "source": [
    "%time xlnet_embeddings, xlnet_vocab = mu.extract_input_embeddings(texts_test, xlnet_model, xlnet_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Baseline model using XLNet pretrained embeddings and vocab*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = BaselineLSTMDescriber(\n",
    "    xlnet_vocab,\n",
    "    embedding=xlnet_embeddings,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antongochev/ai-program/cs224u/repos/xcs224u_final_paper/code/utils/torch_color_describer.py:699: RuntimeWarning: divide by zero encountered in power\n",
      "  perp = [np.prod(s)**(-1/len(s)) for s in scores]\n",
      "Stopping after epoch 61. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 107.04569149017334"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 44min 52s, sys: 1h 13min 37s, total: 3h 58min 29s\n",
      "Wall time: 1h 20min 2s\n"
     ]
    }
   ],
   "source": [
    "%time _ = baseline_model.fit(colors_train, tokens_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.1 s, sys: 5.94 s, total: 48 s\n",
      "Wall time: 41.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'listener_accuracy': 0.8394757000595795, 'corpus_bleu': 0.43106009125838857}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time evaluate(baseline_model, xlnet_tokenizer, raw_colors_test, texts_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on bake-off data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.49 s, sys: 465 ms, total: 6.96 s\n",
      "Wall time: 6.22 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'listener_accuracy': 0.9148202855736091, 'corpus_bleu': 0.6450470273203873}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time evaluate(baseline_model, xlnet_tokenizer, b_raw_colors_test, b_texts_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "roberta_model = RobertaModel.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_train, tokens_train, raw_colors_test, texts_test = create_data(roberta_tokenizer)\n",
    "\n",
    "b_raw_colors_test, b_texts_test = create_bakeoff_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.71 s, sys: 39.2 ms, total: 2.75 s\n",
      "Wall time: 2.75 s\n"
     ]
    }
   ],
   "source": [
    "%time roberta_embeddings, roberta_vocab = mu.extract_input_embeddings(texts_test, roberta_model, roberta_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Baseline model using RoBERTa pretrained embeddings and vocab*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = BaselineLSTMDescriber(\n",
    "    roberta_vocab,\n",
    "    embedding=roberta_embeddings,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 61. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 111.48405265808105"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 41min 37s, sys: 1h 11min 45s, total: 3h 53min 23s\n",
      "Wall time: 1h 15min 32s\n"
     ]
    }
   ],
   "source": [
    "%time _ = baseline_model.fit(colors_train, tokens_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.4 s, sys: 4.02 s, total: 42.4 s\n",
      "Wall time: 35.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'listener_accuracy': 0.8200697931738872, 'corpus_bleu': 0.39418980395247344}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time evaluate(baseline_model, roberta_tokenizer, raw_colors_test, texts_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on bake-off data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.98 s, sys: 511 ms, total: 6.49 s\n",
      "Wall time: 5.65 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'listener_accuracy': 0.8941408173313639, 'corpus_bleu': 0.5649086521910678}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time evaluate(baseline_model, roberta_tokenizer, b_raw_colors_test, b_texts_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELECTRA Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "electra_tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')\n",
    "electra_model = ElectraModel.from_pretrained('google/electra-small-discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_train, tokens_train, raw_colors_test, texts_test = create_data(electra_tokenizer)\n",
    "\n",
    "b_raw_colors_test, b_texts_test = create_bakeoff_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.72 s, sys: 3.16 ms, total: 2.72 s\n",
      "Wall time: 2.72 s\n"
     ]
    }
   ],
   "source": [
    "%time electra_embeddings, electra_vocab = mu.extract_input_embeddings(texts_test, electra_model, electra_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Baseline model using ELECTRA pretrained embeddings and vocab*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = BaselineLSTMDescriber(\n",
    "    electra_vocab,\n",
    "    embedding=electra_embeddings,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 79. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 80.59795188903809"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 2min 9s, sys: 47min 40s, total: 2h 49min 49s\n",
      "Wall time: 56min 50s\n"
     ]
    }
   ],
   "source": [
    "%time _ = baseline_model.fit(colors_train, tokens_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.1 s, sys: 3.48 s, total: 37.6 s\n",
      "Wall time: 32.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'listener_accuracy': 0.8423695633670951, 'corpus_bleu': 0.47554009108512063}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time evaluate(baseline_model, electra_tokenizer, raw_colors_test, texts_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on bake-off data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.45 s, sys: 451 ms, total: 5.9 s\n",
      "Wall time: 5.27 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'listener_accuracy': 0.9187592319054653, 'corpus_bleu': 0.6793120721302932}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time evaluate(baseline_model, electra_tokenizer, b_raw_colors_test, b_texts_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
