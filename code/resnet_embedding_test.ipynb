{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.colors import ColorsCorpusReader\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from collections import Counter\n",
    "import re\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.fft import fft\n",
    "import colorsys\n",
    "from itertools import product\n",
    "from utils.torch_color_describer import (ContextualColorDescriber, create_example_dataset)\n",
    "import utils\n",
    "from utils.utils import UNK_SYMBOL\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline.model import (\n",
    "    BaselineTokenizer, BaselineColorEncoder,\n",
    "    BaselineEmbedding, BaselineDescriber, GloVeEmbedding\n",
    ")\n",
    "from experiment.word_embeddings.helper import Embedding, EmbeddingType\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizer, BertModel,\n",
    "    XLNetTokenizer, XLNetModel,\n",
    "    RobertaTokenizer, RobertaModel,\n",
    "    ElectraTokenizer, ElectraModel,    \n",
    ")\n",
    "\n",
    "import utils.model_utils as mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roberta_embedding(dev_texts, if_contextual=False):\n",
    "    roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    roberta_model = RobertaModel.from_pretrained('roberta-base')    \n",
    "    \n",
    "    roberta_embeddings, roberta_vocab = mu.extract_input_embeddings(dev_texts, roberta_model, roberta_tokenizer)\n",
    "    roberta_contextual_embeddings, roberta_contextual_vocab = mu.extract_positional_embeddings(dev_texts, roberta_model, roberta_tokenizer)\n",
    "    \n",
    "    if not if_contextual:\n",
    "        return (roberta_vocab, roberta_embeddings)\n",
    "    else:\n",
    "        return roberta_contextual_vocab, roberta_contextual_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.utils.fix_random_seeds()\n",
    "COLORS_SRC_FILENAME = os.path.join(\n",
    "    \"data\", \"colors\", \"filteredCorpus.csv\")\n",
    "\n",
    "dev_corpus = ColorsCorpusReader(\n",
    "    COLORS_SRC_FILENAME,\n",
    "    word_count=10,\n",
    "    normalize_colors=True)\n",
    "\n",
    "examples = list(dev_corpus.read())\n",
    "rawcols, texts = zip(*[[ex.colors, ex.contents] for ex in examples])\n",
    "rawcols_train, rawcols_test, texts_train, texts_test = \\\n",
    "    train_test_split(rawcols, texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(424, list, 3, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rawcols), type(rawcols[0]), len(rawcols[0]), len(rawcols[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/yanjiang/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import colorsys\n",
    "\n",
    "def load_model_feature_extractor(model_arch='resnet18'):\n",
    "    model = torch.hub.load('pytorch/vision:v0.6.0', model_arch, pretrained=True)\n",
    "    feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "    return feature_extractor\n",
    "\n",
    "feature_extractor = load_model_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_encoder = BaselineColorEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Color Rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_color_to_rgb(color):\n",
    "    # Convert from HLS to RGB\n",
    "    rgb = colorsys.hls_to_rgb(color[0],color[1],color[2])\n",
    "    return rgb\n",
    "\n",
    "def convert_to_imagenet_input(hsl):\n",
    "    rgb = convert_color_to_rgb(hsl)\n",
    "#     print(\"rgb\", len(rgb))\n",
    "    r = torch.full((224,224),rgb[0]).unsqueeze(2)\n",
    "    g = torch.full((224,224),rgb[1]).unsqueeze(2)\n",
    "    b = torch.full((224,224),rgb[2]).unsqueeze(2)\n",
    "    expanded_rep = torch.cat((r,g,b),2)    \n",
    "#     print(type(expanded_rep), expanded_rep.shape)\n",
    "    expanded_rep = expanded_rep.permute(2,1,0).unsqueeze(0)\n",
    "    return expanded_rep\n",
    "\n",
    "\n",
    "def convert_color_tuple(colors):\n",
    "    converted_colors = [[convert_to_imagenet_input(col) for col in cols] for cols in colors ] \n",
    "    return converted_colors\n",
    "\n",
    "def extract_features_from_batch(extractor, examples):\n",
    "    output = extractor(examples)\n",
    "    shape = output.shape\n",
    "    output = output.reshape((shape[0],shape[1]))\n",
    "#     print(\"shape\", shape, \"reshape\", output.shape)\n",
    "    return output\n",
    "\n",
    "def convert_color_tuple_v2(colors):\n",
    "    extracted_features  = []\n",
    "    with torch.no_grad():\n",
    "        for cols in colors:\n",
    "            cols_group = []\n",
    "            for col in cols:\n",
    "                converted_colors = convert_to_imagenet_input(col)     # [1, 3, 224, 224]\n",
    "    #             print(converted_colors.shape)\n",
    "                cols_group.append(converted_colors)\n",
    "#             print(len(cols_group), cols_group[0].shape) \n",
    "            cols_batch = torch.cat((cols_group[0], cols_group[1], cols_group[2]))\n",
    "            batch_extraction = extract_features_from_batch(feature_extractor, cols_batch)\n",
    "            fourier_embedding = torch.Tensor(color_encoder.encode_color_context(cols))\n",
    "#             print(torch.cat((fourier_embedding, batch_extraction), dim=1).shape)\n",
    "\n",
    "            # convert to numpy array\n",
    "            batch_extraction_np = batch_extraction.numpy()\n",
    "#             print(\"batch_extraction_np\", len(batch_extraction_np), type(batch_extraction_np[0]), batch_extraction_np[0].size)\n",
    "\n",
    "            # append to list\n",
    "            extracted_features.append(batch_extraction_np)\n",
    "\n",
    "            # Print some stats \n",
    "            length = len(extracted_features)\n",
    "            if length%100==0:\n",
    "                total_size = sys.getsizeof(extracted_features)\n",
    "                print(f\"Running batch number: {length}, Size of array: {total_size/(1024**2)} Megabytes\")\n",
    "    return extracted_features        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_extraction: torch.Size([3, 512])\n",
      "raw color <class 'torch.Tensor'> 54\n",
      "torch.Size([3, 566])\n",
      "batch_extraction: torch.Size([3, 512])\n",
      "raw color <class 'torch.Tensor'> 54\n",
      "torch.Size([3, 566])\n"
     ]
    }
   ],
   "source": [
    "converted_data = convert_color_tuple_v2(rawcols[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(converted_data, open( \"data/colors/resnet18_color_embeddings_test.pickle\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_representation(rawcols):\n",
    "    converted_data = convert_color_tuple(rawcols)\n",
    "\n",
    "    extracted_features  = []\n",
    "    with torch.no_grad():\n",
    "        for colors in converted_data:\n",
    "            # Convert to 3x224x224 matrix\n",
    "            print(type(colors), len(colors), colors[0].shape)\n",
    "            cols_batch = torch.cat((colors[0],\n",
    "                                colors[1],\n",
    "                                colors[2]))\n",
    "            print(\"cols_batch:\", cols_batch.shape)\n",
    "\n",
    "            # Run color through the feature extractor\n",
    "            batch_extraction = extract_features_from_batch(feature_extractor, cols_batch)\n",
    "#             print(\"batch_extraction:\", batch_extraction.shape)\n",
    "\n",
    "            # convert to numpy array\n",
    "            batch_extraction_np = batch_extraction.numpy()\n",
    "#             print(\"batch_extraction_np\", len(batch_extraction_np), type(batch_extraction_np[0]), batch_extraction_np[0].size)\n",
    "\n",
    "            # append to list\n",
    "            extracted_features.append(batch_extraction)\n",
    "\n",
    "            # Print some stats \n",
    "            length = len(extracted_features)\n",
    "            if length%100==0:\n",
    "                total_size = sys.getsizeof(extracted_features)\n",
    "                print(f\"Running batch number: {length}, Size of array: {total_size/(1024**2)} Megabytes\")\n",
    "\n",
    "    return extracted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3 torch.Size([1, 3, 224, 224])\n",
      "cols_batch: torch.Size([3, 3, 224, 224])\n",
      "<class 'list'> 3 torch.Size([1, 3, 224, 224])\n",
      "cols_batch: torch.Size([3, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "x = get_color_representation(rawcols[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 512, torch.Tensor)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extracted_features), len(extracted_features[0]), len(extracted_features[0][0]), type(extracted_features[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(tokenizer, include_position=False):    \n",
    "    \n",
    "    rawcols, texts = zip(*[[ex.colors, ex.contents] for ex in examples])\n",
    "\n",
    "    raw_colors_train, raw_colors_test, texts_train, texts_test = \\\n",
    "        train_test_split(rawcols, texts)\n",
    "\n",
    "    tokens_train = [\n",
    "        mu.tokenize_colour_description(text, tokenizer, include_position) for text in texts_train\n",
    "    ]\n",
    "#     color_encoder = BaselineColorEncoder()\n",
    "#     colors_train = [\n",
    "#         color_encoder.encode_color_context(colors) for colors in raw_colors_train\n",
    "#     ]\n",
    "    colors_train = [\n",
    "        get_color_representation(colors) for colors in raw_colors_train\n",
    "    ]\n",
    "\n",
    "    return colors_train, tokens_train, raw_colors_test, texts_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "roberta_model = RobertaModel.from_pretrained('roberta-base') \n",
    "# colors_train, tokens_train, raw_colors_test, texts_test = create_data(roberta_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35245, 3, 54, list)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(colors_train), len(colors_train[0]), len(colors_train[0][0]), type(colors_train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.21 s, sys: 56.2 ms, total: 3.26 s\n",
      "Wall time: 3.41 s\n"
     ]
    }
   ],
   "source": [
    "%time roberta_embeddings, roberta_vocab = mu.extract_input_embeddings(texts_test, roberta_model, roberta_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = BaselineDescriber(\n",
    "    roberta_vocab,\n",
    "    embedding=roberta_embeddings,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 12. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 6.363071918487549"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 460 ms, sys: 37.7 ms, total: 498 ms\n",
      "Wall time: 499 ms\n"
     ]
    }
   ],
   "source": [
    "%time _ = baseline_model.fit(colors_train[:5], tokens_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(trained_model, tokenizer, color_seqs_test, texts_test):\n",
    "    tok_seqs = [mu.tokenize_colour_description(text, tokenizer) for text in texts_test]\n",
    "    col_seqs = [color_encoder.encode_color_context(colors) for colors in color_seqs_test]\n",
    "\n",
    "    return trained_model.evaluate(col_seqs, tok_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'listener_accuracy': 0.3486254149289301, 'corpus_bleu': 0.054725444702242845}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(baseline_model, roberta_tokenizer, raw_colors_test, texts_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
