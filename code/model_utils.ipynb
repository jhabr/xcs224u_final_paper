{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colors import ColorsCorpusReader\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from transformers import pipeline\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    BertTokenizer, BertModel,\n",
    "    XLNetTokenizer, XLNetModel,\n",
    "    RobertaTokenizer, RobertaModel,\n",
    "    ElectraTokenizer, ElectraModel,    \n",
    ")\n",
    "\n",
    "import model_utils as mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS_SRC_FILENAME = os.path.join(\n",
    "    \"data\", \"colors\", \"filteredCorpus.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ColorsCorpusReader(\n",
    "    COLORS_SRC_FILENAME,\n",
    "    word_count=None, #2\n",
    "    normalize_colors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = list(corpus.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46994"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_examples = [example for example in examples if example.condition == \"close\"]\n",
    "split_examples = [example for example in examples if example.condition == \"split\"]\n",
    "far_examples = [example for example in examples if example.condition == \"far\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "close: 15519\n",
      "split: 15693\n",
      "far: 15782\n"
     ]
    }
   ],
   "source": [
    "print(f\"close: {len(close_examples)}\")\n",
    "print(f\"split: {len(split_examples)}\")\n",
    "print(f\"far: {len(far_examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_rawcols, dev_texts = zip(*[[ex.colors, ex.contents] for ex in examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_colours = [\n",
    "    'brown. not the yellow one or classic brown one, the weirder one', \n",
    "    'brown. not the yellow one or classic brown one',\n",
    "    'some other brown. that one'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert model embeddings extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 3058,  119, 1136, 1103, 3431, 1141, 1137, 5263, 3058, 1141,  117,\n",
       "         1103, 6994, 1200, 1141,  102]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.tensor(bert_tokenizer.encode(test_colours[0], add_special_tokens=True)).unsqueeze(0)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'brown',\n",
       " '.',\n",
       " 'not',\n",
       " 'the',\n",
       " 'yellow',\n",
       " 'one',\n",
       " 'or',\n",
       " 'classic',\n",
       " 'brown',\n",
       " 'one',\n",
       " ',',\n",
       " 'the',\n",
       " 'weird',\n",
       " '##er',\n",
       " 'one',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btokens = bert_tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "btokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = mu.extract_input_embeddings(test_colours, bert_model, bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = mu.extract_contextual_embeddings(test_colours, bert_model, bert_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract input embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.01 s, sys: 44.4 ms, total: 9.06 s\n",
      "Wall time: 9.07 s\n"
     ]
    }
   ],
   "source": [
    "%time \\\n",
    "bert_embeddings, bert_vocab = mu.extract_input_embeddings(dev_texts, bert_model, bert_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract contextual embeddings (pre-trained embedding + position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \\\n",
    "bert_contextual_embeddings, bert_contextual_vocab = mu.extract_contextual_embeddings(dev_texts, bert_model, bert_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XLNet model embeddings extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlnet_tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "xlnet_model = XLNetModel.from_pretrained('xlnet-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3442,    9,   50,   18, 3493,   65,   49, 3523, 3442,   65,   19,   18,\n",
       "         8189,  118,   65,    4,    3]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.tensor(xlnet_tokenizer.encode(test_colours[0], add_special_tokens=True)).unsqueeze(0)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁brown',\n",
       " '.',\n",
       " '▁not',\n",
       " '▁the',\n",
       " '▁yellow',\n",
       " '▁one',\n",
       " '▁or',\n",
       " '▁classic',\n",
       " '▁brown',\n",
       " '▁one',\n",
       " ',',\n",
       " '▁the',\n",
       " '▁weird',\n",
       " 'er',\n",
       " '▁one',\n",
       " '<sep>',\n",
       " '<cls>']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest = xlnet_tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = mu.extract_input_embeddings(test_colours, xlnet_model, xlnet_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = mu.extract_contextual_embeddings(test_colours, xlnet_model, xlnet_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract input embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.36 s, sys: 51.2 ms, total: 8.41 s\n",
      "Wall time: 8.42 s\n"
     ]
    }
   ],
   "source": [
    "%time \\\n",
    "xlnet_embeddings, xlnet_vocab = mu.extract_input_embeddings(dev_texts, xlnet_model, xlnet_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract contextual embeddings (pre-trained embedding + position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \\\n",
    "xlnet_contextual_embeddings, xlnet_contextual_vocab = mu.extract_contextual_embeddings(dev_texts, xlnet_model, xlnet_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RoBERTa model embeddings extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "roberta_model = RobertaModel.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 31876,     4,    45,     5,  5718,    65,    50,  4187,  6219,\n",
       "            65,     6,     5,    52,   853,  3624,    65,     2]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.tensor(roberta_tokenizer.encode(test_colours[0], add_special_tokens=True)).unsqueeze(0)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'brown',\n",
       " '.',\n",
       " 'Ġnot',\n",
       " 'Ġthe',\n",
       " 'Ġyellow',\n",
       " 'Ġone',\n",
       " 'Ġor',\n",
       " 'Ġclassic',\n",
       " 'Ġbrown',\n",
       " 'Ġone',\n",
       " ',',\n",
       " 'Ġthe',\n",
       " 'Ġwe',\n",
       " 'ir',\n",
       " 'der',\n",
       " 'Ġone',\n",
       " '</s>']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rtest = roberta_tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "rtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = mu.extract_input_embeddings(test_colours, roberta_model, roberta_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = mu.extract_contextual_embeddings(test_colours, roberta_model, roberta_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract input embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.17 s, sys: 179 ms, total: 9.35 s\n",
      "Wall time: 9.39 s\n"
     ]
    }
   ],
   "source": [
    "%time \\\n",
    "roberta_embeddings, roberta_vocab = mu.extract_input_embeddings(dev_texts, roberta_model, roberta_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract contextual embeddings (pre-trained embedding + position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \\\n",
    "xlnet_contextual_embeddings, xlnet_contextual_vocab = mu.extract_contextual_embeddings(dev_texts, roberta_model, roberta_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ELECTRA model embeddings extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "electra_tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')\n",
    "electra_model = ElectraModel.from_pretrained('google/electra-small-discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 2829, 1012, 2025, 1996, 3756, 2028, 2030, 4438, 2829, 2028, 1010,\n",
       "         1996, 6881, 2121, 2028,  102]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.tensor(electra_tokenizer.encode(test_colours[0], add_special_tokens=True)).unsqueeze(0)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'brown',\n",
       " '.',\n",
       " 'not',\n",
       " 'the',\n",
       " 'yellow',\n",
       " 'one',\n",
       " 'or',\n",
       " 'classic',\n",
       " 'brown',\n",
       " 'one',\n",
       " ',',\n",
       " 'the',\n",
       " 'weird',\n",
       " '##er',\n",
       " 'one',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rtest = electra_tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "rtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = mu.extract_input_embeddings(test_colours, electra_model, electra_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = mu.extract_contextual_embeddings(test_colours, electra_model, electra_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract input embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.7 s, sys: 56.6 ms, total: 10.7 s\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%time \\\n",
    "roberta_embeddings, roberta_vocab = mu.extract_input_embeddings(dev_texts, electra_model, electra_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract contextual embeddings (pre-trained embedding + position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \\\n",
    "xlnet_contextual_embeddings, xlnet_contextual_vocab = mu.extract_contextual_embeddings(dev_texts, electra_model, electra_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate token sequences based on raw colour descriptions converted into model based tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'brown',\n",
       " 'not',\n",
       " 'the',\n",
       " 'yellow',\n",
       " 'one',\n",
       " 'or',\n",
       " 'classic',\n",
       " 'brown',\n",
       " 'one',\n",
       " 'the',\n",
       " 'weird',\n",
       " 'er',\n",
       " 'one',\n",
       " '</s>']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu.tokenize_colour_description(test_colours[0], tokenizer=electra_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
