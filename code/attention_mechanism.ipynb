{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from utils.colors import ColorsCorpusReader\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from utils.torch_color_describer import (\n",
    "    ContextualColorDescriber, create_example_dataset\n",
    ")\n",
    "import utils.utils as utils\n",
    "from utils.utils import UNK_SYMBOL\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatch\n",
    "import numpy as np\n",
    "from baseline.model import (\n",
    "    BaselineTokenizer, BaselineColorEncoder,\n",
    "    BaselineEmbedding, BaselineDescriber, GloVeEmbedding\n",
    ")\n",
    "from experiment.word_embeddings.helper import Embedding, EmbeddingType\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COLORS_SRC_FILENAME = os.path.join(\n",
    "    \"data\", \"colors\", \"filteredCorpus.csv\"\n",
    ")\n",
    "corpus = ColorsCorpusReader(\n",
    "    COLORS_SRC_FILENAME,\n",
    "    word_count=10,\n",
    "    normalize_colors=True\n",
    ")\n",
    "examples = list(corpus.read())\n",
    "examples = examples[:10]\n",
    "len(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bahdanau Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.torch_color_describer import Encoder, Decoder, EncoderDecoder \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "tokenizer = BaselineTokenizer()\n",
    "color_encoder = BaselineColorEncoder()\n",
    "embedding = BaselineEmbedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Producing the Encoder Hidden States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_withAttention(Encoder):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def forward(self, color_seqs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        color_seqs : torch.FloatTensor\n",
    "            The shape is `(m, n, p)` where `m` is the batch_size,\n",
    "             `n` is the number of colors in each context, and `p` is\n",
    "             the color dimensionality.\n",
    "        Returns\n",
    "        -------\n",
    "        hidden : torch.FloatTensor\n",
    "            These are the final hidden state of the RNN for this batch,\n",
    "            shape `(m, p) where `m` is the batch_size and `p` is\n",
    "             the color dimensionality.\n",
    "        output : tensor containing the output features h_t from the last layer of the GRU \n",
    "        \"\"\"\n",
    "        \n",
    "        output, hidden = self.rnn(color_seqs)\n",
    "        print(\"color_seqs\", color_seqs.shape, \"hidden\", hidden.shape\n",
    "              , \"output\", output.shape)  # hidden.shape is (1, m, p)\n",
    "        return output, hidden   # what is the output vs hidden?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Step 2 to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_withAttention(Decoder):\n",
    "    def __init__(self, color_dim, *args, **kwargs):\n",
    "        self.color_dim = color_dim\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.fc_hidden = nn.Linear(self.hidden_dim, self.hidden_dim, bias=False)\n",
    "        self.fc_encoder = nn.Linear(self.hidden_dim, self.hidden_dim, bias=False)\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(1, self.hidden_dim))\n",
    "\n",
    "\n",
    "    def get_embeddings(self, word_seqs, target_colors=None):\n",
    "        \"\"\"\n",
    "        You can assume that `target_colors` is a tensor of shape\n",
    "        (m, n), where m is the length of the batch (same as\n",
    "        `word_seqs.shape[0]`) and n is the dimensionality of the\n",
    "        color representations the model is using. The goal is\n",
    "        to attached each color vector i to each of the tokens in\n",
    "        the ith sequence of (the embedded version of) `word_seqs`.\n",
    "        \"\"\"\n",
    "        \n",
    "        target_colors = torch.unsqueeze(target_colors, 1)\n",
    "        target_colors_token = torch.repeat_interleave(target_colors, word_seqs.shape[1], dim=1)\n",
    "        ret = torch.cat((self.embedding(word_seqs), target_colors_token), 2)\n",
    "        return ret\n",
    "    \n",
    "    \n",
    "    def forward(self, word_seqs, seq_lengths=None, hidden=None, target_colors=None, encoder_outputs=None):\n",
    "        encoder_outputs = encoder_outputs.squeeze()   # (6, 3, 50)   \n",
    "        embs = self.get_embeddings(word_seqs, target_colors=target_colors)\n",
    "        print(\"Decoder encoder_output\", encoder_outputs.shape, \"word_seqs\", word_seqs.shape)\n",
    "        \n",
    "        # Calculating Alignment Scores (final score (6, 3, 1)\n",
    "        # hidden: 1x batch_size x color_dim   (1x6x50)\n",
    "        # encoder_output: batch_size x num_colors x color_dim (6x3x50)\n",
    "        print(\"hidden shape\", hidden.shape, hidden[0].shape, encoder_outputs[0].shape)  \n",
    "        print(\"linear layer1\", self.fc_hidden(hidden[0]).shape)\n",
    "        x = torch.tanh(self.fc_hidden(hidden[0])+self.fc_encoder(encoder_outputs[0]))\n",
    "\n",
    "        \n",
    "        \n",
    "        if self.training:\n",
    "            # Packed sequence for performance:\n",
    "            embs = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "                embs,\n",
    "                batch_first=True,\n",
    "                lengths=seq_lengths,\n",
    "                enforce_sorted=False)\n",
    "            # RNN forward:\n",
    "            print(\"before rnn\", len(embs),len(embs[0]), len(embs[0][0]), hidden.shape)\n",
    "            output, hidden = self.rnn(embs, hidden)\n",
    "            print(\"rnn ok?\")\n",
    "            # Unpack:\n",
    "            output, seq_lengths = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "                output, batch_first=True)\n",
    "            # Output dense layer to get logits:\n",
    "            output = self.output_layer(output)\n",
    "            # Drop the final element:\n",
    "            output = output[:, : -1, :]\n",
    "            # Reshape for the sake of the loss function:\n",
    "            output = output.transpose(1, 2)\n",
    "            return output, hidden\n",
    "        else:\n",
    "            output, hidden = self.rnn(embs, hidden)\n",
    "            output = self.output_layer(output)\n",
    "            return output, hidden   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizedEncoderDecoder(EncoderDecoder):\n",
    "\n",
    "    def forward(self,\n",
    "            color_seqs,\n",
    "            word_seqs,\n",
    "            seq_lengths=None,\n",
    "            hidden=None,\n",
    "            targets=None):\n",
    "        if hidden is None:\n",
    "            encoder_output, encoder_hidden = self.encoder(color_seqs)\n",
    "\n",
    "#         print(\"ED forward color seq\", color_seqs.shape) \n",
    "#         print (\"ED\", encoder_output.shape, encoder_hidden.shape)\n",
    "        \n",
    "        output, hidden = self.decoder.forward(\n",
    "            word_seqs, seq_lengths=seq_lengths, hidden=encoder_hidden, \n",
    "            target_colors=color_seqs[:,-1], encoder_outputs= encoder_output)\n",
    "\n",
    "        if self.training:\n",
    "            return output\n",
    "        else:\n",
    "            return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizedInputDescriber(ContextualColorDescriber):\n",
    "\n",
    "    def build_graph(self):\n",
    "        encoder = Encoder_withAttention(\n",
    "            color_dim=self.color_dim,\n",
    "            hidden_dim=self.hidden_dim)\n",
    "\n",
    "        decoder = Decoder_withAttention(\n",
    "         vocab_size=self.vocab_size,\n",
    "            embed_dim=self.embed_dim,\n",
    "            embedding=self.embedding,\n",
    "            hidden_dim=50,\n",
    "            color_dim=self.color_dim\n",
    "        )      \n",
    "\n",
    "        encoder_decoder = ColorizedEncoderDecoder(\n",
    "            encoder = encoder,\n",
    "            decoder = decoder)\n",
    "        \n",
    "        return encoder_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describler build dataset 6\n",
      "ColorDataset _init_ 6\n",
      "color_seqs torch.Size([6, 3, 54]) hidden torch.Size([1, 6, 50]) output torch.Size([6, 3, 50])\n",
      "embs torch.Size([6, 12, 104])\n",
      "Decoder encoder_output torch.Size([6, 3, 50]) word_seqs torch.Size([6, 12])\n",
      "hidden shape torch.Size([1, 6, 50]) torch.Size([6, 50]) torch.Size([3, 50])\n",
      "linear layer1 torch.Size([6, 50])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6) must match the size of tensor b (3) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Classes/CS224/xcs224u_final_paper/code/utils/torch_model_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m                 \u001b[0mbatch_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlu/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-251-ddc9bfa3d463>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, color_seqs, word_seqs, seq_lengths, hidden, targets)\u001b[0m\n\u001b[1;32m     15\u001b[0m         output, hidden = self.decoder.forward(\n\u001b[1;32m     16\u001b[0m             \u001b[0mword_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             target_colors=color_seqs[:,-1], encoder_outputs= encoder_output)\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-250-dac97dd0db0a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, word_seqs, seq_lengths, hidden, target_colors, encoder_outputs)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hidden shape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"linear layer1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (6) must match the size of tensor b (3) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "def create_data():    \n",
    "    rawcols, texts = zip(*[[ex.colors, ex.contents] for ex in examples])\n",
    "\n",
    "    raw_colors_train, raw_colors_test, texts_train, texts_test = \\\n",
    "        train_test_split(rawcols, texts)\n",
    "\n",
    "    tokens_train = [tokenizer.encode(text) for text in texts_train]\n",
    "    colors_train = [\n",
    "        color_encoder.encode_color_context(colors) for colors in raw_colors_train\n",
    "    ]\n",
    "\n",
    "    vocab = sorted({word for tokens in tokens_train for word in tokens})\n",
    "    vocab += [UNK_SYMBOL]\n",
    "\n",
    "    return vocab, colors_train, tokens_train, raw_colors_test, texts_test\n",
    "\n",
    "vocab, colors_train, tokens_train, raw_colors_test, texts_test = create_data()\n",
    "\n",
    "glove_embedding, glove_vocab = embedding.create_embeddings(vocab)\n",
    "\n",
    "baseline_model = ColorizedInputDescriber(\n",
    "    vocab=glove_vocab,\n",
    "    embedding=glove_embedding,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
