{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__authors__ = \"Anton Gochev, Jaro Habr, Yan Jiang, Samuel Kahn\"\n",
    "__version__ = \"XCS224u, Stanford, Spring 2021\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colours with contextual embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the classes and helpers from the [model.py](experiment/model.py) library. Here we experiment with different combinations of contextual embeddings aiming to determine which will have a better performance than the basemodel or the static embeddings used in [colours_with_static_embeddings_with_convolutional_image_embeddings.ipynb](colours_with_static_embeddings_with_convolutional_image_embeddings).\n",
    "\n",
    "## WIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.colors import ColorsCorpusReader\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils.torch_color_describer import (\n",
    "    ContextualColorDescriber,\n",
    "    create_example_dataset\n",
    ")\n",
    "\n",
    "from utils.utils import START_SYMBOL, END_SYMBOL, UNK_SYMBOL\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizer, BertModel,\n",
    "    XLNetConfig, XLNetModel,\n",
    "    XLNetTokenizer, XLNetForSequenceClassification,\n",
    "    RobertaTokenizer, RobertaModel,\n",
    "    ElectraTokenizer, ElectraModel,\n",
    "    EncoderDecoderModel\n",
    ")\n",
    "import utils.model_utils as mu\n",
    "import experiment.helper as helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exploration of the dataset counts the examples for different classes and plots the words distribition in order to see any data imbalance issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtered Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filtered corpus is the full dataset used in assignment 4. The following code looks at the composition of the dataset, the number of example in each condition as well as the word count used in the color descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS_SRC_FILENAME = os.path.join(\n",
    "    \"data\", \"colors\", \"filteredCorpus.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ColorsCorpusReader(\n",
    "    COLORS_SRC_FILENAME,\n",
    "    word_count=None,\n",
    "    normalize_colors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = list(corpus.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46994"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the datasets (training and bake-off) in more details refer to [colors_in_context.ipynb](colors_in_context.ipynb). The notebook shows the distribution of the colours examples among the different splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bake-Off Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code analyses the bake-off dataset. We will look at the number of examples for each of the conditions as well as the word count used to described the colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAKE_OFF_COLORS_SRC_FILENAME = os.path.join(\n",
    "    \"data\", \"colors\", \"cs224u-colors-bakeoff-data.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bake_off_corpus = ColorsCorpusReader(\n",
    "    BAKE_OFF_COLORS_SRC_FILENAME,\n",
    "    word_count=None,\n",
    "    normalize_colors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bake_off_examples = list(bake_off_corpus.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2031"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bake_off_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline-System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This baseline system is based on assignment 4 and is enhanced with new classes that allow using different contextual embedding extrsactors for easier experiments and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from baseline.model import BaselineColorEncoder\n",
    "\n",
    "from experiment.model import (\n",
    "    TransformerEmbeddingDecoder, \n",
    "    TransformerEmbeddingDescriber,\n",
    "    EmbeddingExtractorType,\n",
    "     TransformerType\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_encoder = BaselineColorEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full dataset used for training and expeiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(tokenizer):\n",
    "    rawcols, texts = zip(*[[ex.colors, ex.contents] for ex in examples])\n",
    "    \n",
    "    raw_colors_train, raw_colors_test, texts_train, texts_test = \\\n",
    "        train_test_split(rawcols, texts)\n",
    "    \n",
    "    raw_colors_train = raw_colors_train\n",
    "    texts_train = texts_train\n",
    "\n",
    "    tokens_train = [\n",
    "        mu.tokenize_colour_description(text, tokenizer) for text in texts_train\n",
    "    ]\n",
    "    colors_train = [\n",
    "        color_encoder.encode_color_context(colors) for colors in raw_colors_train\n",
    "    ]\n",
    "\n",
    "    return colors_train, tokens_train, raw_colors_test, texts_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bakeoff_data():    \n",
    "    return zip(*[[ex.colors, ex.contents] for ex in bake_off_examples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from the experiments:\n",
    "\n",
    "| Model | Hidden Layer | Protocol | Training Results | Bake-off Results |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| BERT 'bert-base-cased' | Layer 12 | Stopping after epoch 13. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 6.030726432800293 CPU times: user 1h 43min 56s, sys: 54min 44s, total: 2h 38min 40s Wall time: 1h 22min 48s | {'listener_accuracy': 0.2905405405405405, 'corpus_bleu': 0.07466216216216218} | {'listener_accuracy': 0.32348596750369274, 'corpus_bleu': 0.050147710487444604} |\n",
    "| BERT 'bert-base-cased' | Positional | Stopping after epoch 27. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 5.257309436798096 CPU times: user 1h 23min 10s, sys: 10min 23s, total: 1h 33min 33s Wall time: 58min 54s | {'listener_accuracy': 0.32432432432432434, 'corpus_bleu': 0.2281656643331077} | {'listener_accuracy': 0.3559822747415066, 'corpus_bleu': 0.39051130299796255} |\n",
    "| BERT 'bert-base-cased' fixed padding and baseline special symbols | Layer 12 | Stopping after epoch 14. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 6.11735725402832 CPU times: user 1h 44min 47s, sys: 54min 54s, total: 2h 39min 42s Wall time: 1h 21min 23s | {'listener_accuracy': 0.36486486486486486, 'corpus_bleu': 0.035602020040005664} | {'listener_accuracy': 0.3658296405711472, 'corpus_bleu': 0.41658000242602683} |\n",
    "| BERT 'bert-base-cased' fixed padding and BERT special symbols | Layer 12 | Stopping after epoch 15. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 5.931732177734375 CPU times: user 1h 53min 23s, sys: 59min 28s, total: 2h 52min 52s Wall time: 1h 29min 26s | {'listener_accuracy': 0.34459459459459457, 'corpus_bleu': 0.05016891891891893} | {'listener_accuracy': 0.3293943870014771, 'corpus_bleu': 0.050147710487444604} | \n",
    "| BERT 'bert-base-cased' | Layer 12 + static | Stopping after epoch 12. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 6.027207374572754 CPU times: user 1h 48min 15s, sys: 55min 29s, total: 2h 43min 44s Wall time: 1h 26min 43s | {'listener_accuracy': 0.28716216216216217, 'corpus_bleu': 0.050337837837837844} | {'listener_accuracy': 0.35942885278188086, 'corpus_bleu': 0.05009847365829641} |\n",
    "| XLNet 'xlnet-base-cased' |  |  |  |  |\n",
    "| RoBERTa 'roberta-base' |  |  |  |  |\n",
    "| ELECTRA 'google/electra-small-discriminator' |  |  |  |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial experiments show that using just contextual or positional embeddings is certainly not improving the performance. It seems that those are very high-level and we are missing on some of the low level context static embeddings provide us with. Therefore, next steps are to implement and text embeddings extractors that make use of the static embeddings. \n",
    "- Do not throw away the static embeddings as they have good performance\n",
    "- Combine the static embeddings with the [CLS] output to see what the result is (he had good experience with such approach)\n",
    "- Combine the static embeddings with the output of the model (contextual embeddings/last hidden state) to see if there is any improvement\n",
    "\n",
    "In additoin, we will try with different model complexity (increase the hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_raw_colors_test, b_texts_test = create_bakeoff_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.78 s, sys: 79.1 ms, total: 7.86 s\n",
      "Wall time: 7.86 s\n"
     ]
    }
   ],
   "source": [
    "%time  colors_train, tokens_train, raw_colors_test, texts_test = create_data(bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, vocab = mu.extract_input_embeddings(texts_test, bert_model, bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerEmbeddingDescriber(\n",
    "    vocab=vocab, \n",
    "    embedding=embeddings, \n",
    "    transformer=TransformerType.BERT, \n",
    "    extractor=EmbeddingExtractorType.STATICANDLAYER12, \n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 12. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 6.800752639770508"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.8 s, sys: 18.6 s, total: 1min 8s\n",
      "Wall time: 31.6 s\n"
     ]
    }
   ],
   "source": [
    "%time _ = model.fit(colors_train[:5], tokens_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate on test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.15 s, sys: 250 ms, total: 4.4 s\n",
      "Wall time: 4.22 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'listener_accuracy': 0.2, 'corpus_bleu': 0.05000000000000001}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time evaluate(model, bert_tokenizer, raw_colors_test[:5], texts_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate with bake-off data the model hasn't seen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.25 s, sys: 270 ms, total: 4.52 s\n",
      "Wall time: 4.32 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'listener_accuracy': 0.4, 'corpus_bleu': 0.05000000000000001}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time evaluate(model, bert_tokenizer, b_raw_colors_test[:5], b_texts_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLNet Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlnet_tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "xlnet_model = XLNetModel.from_pretrained('xlnet-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.38 s, sys: 196 ms, total: 7.58 s\n",
      "Wall time: 7.58 s\n"
     ]
    }
   ],
   "source": [
    "%time  colors_train, tokens_train, raw_colors_test, texts_test = create_data(xlnet_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, vocab = mu.extract_input_embeddings(texts_test, xlnet_model, xlnet_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerEmbeddingDescriber(\n",
    "    vocab=vocab, \n",
    "    embedding=embeddings, \n",
    "    transformer=TransformerType.XLNet, \n",
    "    extractor=EmbeddingExtractorType.STATICANDLAYER12, \n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 12. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 7.029555320739746"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.5 s, sys: 24.4 s, total: 1min 18s\n",
      "Wall time: 35.3 s\n"
     ]
    }
   ],
   "source": [
    "%time _ = model.fit(colors_train[:5], tokens_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate on test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.57 s, sys: 147 ms, total: 1.72 s\n",
      "Wall time: 1.61 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'listener_accuracy': 0.4, 'corpus_bleu': 0.05000000000000001}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time helper.evaluate(model, xlnet_tokenizer, raw_colors_test[:5], texts_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate with bake-off data the model hasn't seen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.44 s, sys: 146 ms, total: 1.58 s\n",
      "Wall time: 1.47 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'listener_accuracy': 0.2, 'corpus_bleu': 0.05000000000000001}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time helper.evaluate(model, xlnet_tokenizer, b_raw_colors_test[:5], b_texts_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "roberta_model = RobertaModel.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.47 s, sys: 504 ms, total: 8.97 s\n",
      "Wall time: 9.15 s\n"
     ]
    }
   ],
   "source": [
    "%time  colors_train, tokens_train, raw_colors_test, texts_test = create_data(roberta_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, vocab = mu.extract_input_embeddings(texts_test, roberta_model, roberta_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerEmbeddingDescriber(\n",
    "    vocab=vocab, \n",
    "    embedding=embeddings, \n",
    "    transformer=TransformerType.RoBERTa, \n",
    "    extractor=EmbeddingExtractorType.STATICANDLAYER12, \n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 14. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 6.2082929611206055"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 5s, sys: 28.7 s, total: 1min 34s\n",
      "Wall time: 39.3 s\n"
     ]
    }
   ],
   "source": [
    "%time _ = model.fit(colors_train[:5], tokens_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate on test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.33 s, sys: 122 ms, total: 1.46 s\n",
      "Wall time: 1.36 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'listener_accuracy': 0.6, 'corpus_bleu': 0.05000000000000001}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time helper.evaluate(model, roberta_tokenizer, raw_colors_test[:5], texts_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate with bake-off data the model hasn't seen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.47 s, sys: 149 ms, total: 1.62 s\n",
      "Wall time: 1.51 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'listener_accuracy': 0.2, 'corpus_bleu': 0.05000000000000001}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time helper.evaluate(model, roberta_tokenizer, b_raw_colors_test[:5], b_texts_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELECTRA Embeddings WIP - doesn't work yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "electra_tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')\n",
    "electra_model = ElectraModel.from_pretrained('google/electra-small-discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.16 s, sys: 427 ms, total: 9.59 s\n",
      "Wall time: 9.76 s\n"
     ]
    }
   ],
   "source": [
    "%time  colors_train, tokens_train, raw_colors_test, texts_test = create_data(electra_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, vocab = mu.extract_input_embeddings(texts_test, electra_model, electra_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerEmbeddingDescriber(\n",
    "    vocab=vocab, \n",
    "    embedding=embeddings, \n",
    "    transformer=TransformerType.ELECTRA, \n",
    "    extractor=EmbeddingExtractorType.STATICANDLAYER12, \n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 12. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 5.5499653816223145"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.47 s, sys: 2.21 s, total: 8.67 s\n",
      "Wall time: 6.27 s\n"
     ]
    }
   ],
   "source": [
    "%time _ = model.fit(colors_train[:5], tokens_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate on test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.39 s, sys: 122 ms, total: 1.51 s\n",
      "Wall time: 1.42 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'listener_accuracy': 0.2, 'corpus_bleu': 0.05000000000000001}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time helper.evaluate(model, electra_tokenizer, raw_colors_test[:5], texts_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate with bake-off data the model hasn't seen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.38 s, sys: 133 ms, total: 1.51 s\n",
      "Wall time: 1.41 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'listener_accuracy': 0.4, 'corpus_bleu': 0.05000000000000001}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time helper.evaluate(model, electra_tokenizer, b_raw_colors_test[:5], b_texts_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
